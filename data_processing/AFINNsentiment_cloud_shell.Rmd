---
title: "AFINN and Word Cloud sentiment template"
date: '`r Sys.Date()`'

---

# Set working directoryLoad libraries
Let us start by installing and loading tidyr package.


```{r setup, include=FALSE}

# The easiest way to get tidyr is to install the whole tidyverse:
#install.packages("knitr", repos="http://cran.rstudio.com/")
#install.packages("GGally", repos="http://cran.rstudio.com/")

# Alternatively, install just tidyr:
#install.packages("tidyr")

# Or the the development version from GitHub:
# install.packages("devtools")
#devtools::install_github("tidyverse/tidyr")

#install.packages("wordcloud")

#After installing, we need to call the package to make it available to R
library(readr)
library(stargazer)
library(knitr)
library(dplyr)
#library(GGally)
#library("psych")
library(ggplot2)
library(stringr)
#library("ggExtra")
library(psych)



library(dplyr)
library(tidyr)
library(purrr)
library(readr)

install.packages("topicmodels")
library(tidytext)
#library(widyr)
#library(ggraph)
library(igraph)
library(tm)
library(topicmodels)

library(wordcloud)
library(reshape2)
#library("ldatuning")

#Stemming
#https://github.com/juliasilge/tidytext/issues/17
library(SnowballC)
library(RCurl)
```   




```{r}
#read in the speeches using RCurl package direct from Github       
      
preslink <-getURL("https://raw.githubusercontent.com/bwaite15104/bwaite15104.github.io/master/data/presidential_inaugural_speeches.csv")

allpres<-read.csv(text = preslink,header=TRUE, sep=",", stringsAsFactors = FALSE)

```


## Tidy text
Clean up text so that we can get it ready for analysis

```{r}
#Remove stop words

tidy_pres <- allpres %>%
              unnest_tokens(word, speech_text) %>% 
              anti_join(stop_words) %>% 
              filter(word != "br") %>% #HTML tag <br /><br /> results in the word "br"
              mutate(word = wordStem(word))


#View(head(tidy_amzn,1000))
#glimpse(tidy_amzn)
# $ rev_id    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...
# $ productId <chr> "B001E4KFG0", "B001E4KFG0", "B001E4KFG0", "B001E4KFG0", "B001E4KFG0", "B001E4KFG0...
# $ userId    <chr> "A3SGXH7AUHU8GW", "A3SGXH7AUHU8GW", "A3SGXH7AUHU8GW", "A3SGXH7AUHU8GW", "A3SGXH7A...
# $ score     <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
# $ time      <dbl> 1303862400, 1303862400, 1303862400, 1303862400, 1303862400, 1303862400, 130386240...
# $ word      <chr> "bought", "vitality", "canned", "dog", "food", "products", "found", "quality", "p...



```

## Word count analysis

```{r}
#print top 10 words
tidy_pres %>% 
count(word, sort = TRUE) %>% 
  slice(1:10)


```
#bar chart showing most used words by word count
tidy_pres %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()


```

## Word cloud

```{r}

#need to run sentiment analysis first

#overall word cloud
tidy_pres %>% 
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

#using AFINN scoring
badword<-(tidy_pres_sentimentA[tidy_pres_sentimentA$score<0,])
goodword<-(tidy_pres_sentimentA[tidy_pres_sentimentA$score>0,])

#word cloud of the positive words
goodword %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))


#word cloud of the negative words
badword %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```



## Sentiment analysis

```{r}

#The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating #negative sentiment and positive scores indicating positive sentiment. 

# > get_sentiments("afinn")
# # A tibble: 2,476 x 2
#    word       score
#    <chr>      <int>
#  1 abandon       -2
#  2 abandoned     -2
#  3 abandons      -2
#  4 abducted      -2
#  5 abduction     -2
#  6 abductions    -2
#  7 abhor         -3
#  8 abhorred      -3
#  9 abhorrent     -3
# 10 abhors        -3


#BING gives 
tidy_pres_sentimentB <- tidy_pres %>%
                          inner_join(get_sentiments("bing"), by = "word")

#AFINN gives
tidy_pres_sentimentA <- tidy_pres %>%
                          inner_join(get_sentiments("afinn"), by = "word")

#this is the average sentiment score across all of the stemmed words
mean(tidy_pres_sentimentA$score)

#calculate sentiment score for BING
sent_table-table(tidy_pres_sentimentB$sentiment)
BingScore<-sent_table[1]/sent_table[2]


#Now get AFINN average sentiment score for each productId to plot rating vs. avg_score
tidy_pres_sentimentA_pres <- tidy_pres_sentimentA %>% 
                            group_by(president_name) %>% 
                              summarise(avg_score=mean(score),
                                        sum_score=sum(score))
    

ggplot(tidy_pres_sentimentA_pres, aes(x=president_name, y=avg_score))

```

```{r}
#ggplot(tidy_amzn_sentiment_prod, aes(x=avg_score, y=avg_rating)) +
#    geom_point(shape=1) +    # Use hollow circles
#    geom_smooth(method=lm,   # Add linear regression line
#                se=TRUE)    # Don't add shaded confidence region


```
## Topic Modelling

Latent Dirichlet allocation (LDA) is one of the most common algorithms for topic modeling. Without diving into the math behind the model, we can understand it as being guided by two principles.

Every document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”
Every topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.

Right now our data frame word_counts is in a tidy form, with one-term-per-document-per-row, but the topicmodels package requires a DocumentTermMatrix. As described in Chapter 5.2, we can cast a one-token-per-row table into a DocumentTermMatrix with tidytext’s cast_dtm().

```{r}
#cast to DocumentTermMatrix
pres_dtm <- tidy_pres %>%
                count(president_name, word, sort = TRUE) %>%
                ungroup() %>%
                cast_dtm(president_name, word, n)


# 5 topics
pres_lda <- LDA(pres_dtm, k = 5, control = list(seed=1234))

pres_topics <-tidy(pres_lda,matrix ="beta")

top_terms <- pres_topics %>%
                group_by(topic) %>%
                top_n(5, beta) %>%
                ungroup() %>%
                arrange(topic, -beta)


top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()


```

